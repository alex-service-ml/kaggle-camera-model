{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN\n",
    "Goal is to see how far we can get with a ReLU-based CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CameraLabel(Enum):\n",
    "    HTC_1_M7 = 0\n",
    "    LG_Nexus_5x = 1\n",
    "    Motorola_Droid_Maxx = 2\n",
    "    Motorola_Nexus_6 = 3\n",
    "    Motorola_X = 4\n",
    "    Samsung_Galaxy_Note3 = 5\n",
    "    Samsung_Galaxy_S4 = 6\n",
    "    Sony_NEX_7 = 7\n",
    "    iPhone_4s = 8\n",
    "    iPhone_6 = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path='data-224'):\n",
    "    data = []\n",
    "    labels = []\n",
    "    # Read data from every type of generated data\n",
    "    for data_path in os.listdir(path):\n",
    "        full_data_path = os.path.join(path, data_path)\n",
    "        # Read data from every camera type\n",
    "        for label in CameraLabel:\n",
    "            camera_path = os.path.join(full_data_path, label.name.replace('_', '-'))\n",
    "            print(\"Loading images for \", camera_path)\n",
    "            for filename in os.listdir(camera_path):\n",
    "                data += [cv2.imread(os.path.join(camera_path, filename))]\n",
    "                labels += [CameraLabel[label.name].value]\n",
    "    return np.asarray(data), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images for  data-224/train_resize_5/HTC-1-M7\n",
      "Loading images for  data-224/train_resize_5/LG-Nexus-5x\n",
      "Loading images for  data-224/train_resize_5/Motorola-Droid-Maxx\n",
      "Loading images for  data-224/train_resize_5/Motorola-Nexus-6\n",
      "Loading images for  data-224/train_resize_5/Motorola-X\n",
      "Loading images for  data-224/train_resize_5/Samsung-Galaxy-Note3\n",
      "Loading images for  data-224/train_resize_5/Samsung-Galaxy-S4\n",
      "Loading images for  data-224/train_resize_5/Sony-NEX-7\n",
      "Loading images for  data-224/train_resize_5/iPhone-4s\n",
      "Loading images for  data-224/train_resize_5/iPhone-6\n",
      "Loading images for  data-224/train_gamma_8/HTC-1-M7\n",
      "Loading images for  data-224/train_gamma_8/LG-Nexus-5x\n",
      "Loading images for  data-224/train_gamma_8/Motorola-Droid-Maxx\n",
      "Loading images for  data-224/train_gamma_8/Motorola-Nexus-6\n",
      "Loading images for  data-224/train_gamma_8/Motorola-X\n",
      "Loading images for  data-224/train_gamma_8/Samsung-Galaxy-Note3\n",
      "Loading images for  data-224/train_gamma_8/Samsung-Galaxy-S4\n",
      "Loading images for  data-224/train_gamma_8/Sony-NEX-7\n",
      "Loading images for  data-224/train_gamma_8/iPhone-4s\n",
      "Loading images for  data-224/train_gamma_8/iPhone-6\n",
      "Loading images for  data-224/train_gamma_12/HTC-1-M7\n",
      "Loading images for  data-224/train_gamma_12/LG-Nexus-5x\n",
      "Loading images for  data-224/train_gamma_12/Motorola-Droid-Maxx\n",
      "Loading images for  data-224/train_gamma_12/Motorola-Nexus-6\n",
      "Loading images for  data-224/train_gamma_12/Motorola-X\n",
      "Loading images for  data-224/train_gamma_12/Samsung-Galaxy-Note3\n",
      "Loading images for  data-224/train_gamma_12/Samsung-Galaxy-S4\n",
      "Loading images for  data-224/train_gamma_12/Sony-NEX-7\n",
      "Loading images for  data-224/train_gamma_12/iPhone-4s\n",
      "Loading images for  data-224/train_gamma_12/iPhone-6\n",
      "Loading images for  data-224/train_resize_8/HTC-1-M7\n",
      "Loading images for  data-224/train_resize_8/LG-Nexus-5x\n",
      "Loading images for  data-224/train_resize_8/Motorola-Droid-Maxx\n",
      "Loading images for  data-224/train_resize_8/Motorola-Nexus-6\n",
      "Loading images for  data-224/train_resize_8/Motorola-X\n",
      "Loading images for  data-224/train_resize_8/Samsung-Galaxy-Note3\n",
      "Loading images for  data-224/train_resize_8/Samsung-Galaxy-S4\n",
      "Loading images for  data-224/train_resize_8/Sony-NEX-7\n",
      "Loading images for  data-224/train_resize_8/iPhone-4s\n",
      "Loading images for  data-224/train_resize_8/iPhone-6\n",
      "Loading images for  data-224/train_compressed_70/HTC-1-M7\n",
      "Loading images for  data-224/train_compressed_70/LG-Nexus-5x\n",
      "Loading images for  data-224/train_compressed_70/Motorola-Droid-Maxx\n",
      "Loading images for  data-224/train_compressed_70/Motorola-Nexus-6\n",
      "Loading images for  data-224/train_compressed_70/Motorola-X\n",
      "Loading images for  data-224/train_compressed_70/Samsung-Galaxy-Note3\n",
      "Loading images for  data-224/train_compressed_70/Samsung-Galaxy-S4\n",
      "Loading images for  data-224/train_compressed_70/Sony-NEX-7\n",
      "Loading images for  data-224/train_compressed_70/iPhone-4s\n",
      "Loading images for  data-224/train_compressed_70/iPhone-6\n",
      "Loading images for  data-224/train_cropped/HTC-1-M7\n",
      "Loading images for  data-224/train_cropped/LG-Nexus-5x\n",
      "Loading images for  data-224/train_cropped/Motorola-Droid-Maxx\n",
      "Loading images for  data-224/train_cropped/Motorola-Nexus-6\n",
      "Loading images for  data-224/train_cropped/Motorola-X\n",
      "Loading images for  data-224/train_cropped/Samsung-Galaxy-Note3\n",
      "Loading images for  data-224/train_cropped/Samsung-Galaxy-S4\n",
      "Loading images for  data-224/train_cropped/Sony-NEX-7\n",
      "Loading images for  data-224/train_cropped/iPhone-4s\n",
      "Loading images for  data-224/train_cropped/iPhone-6\n",
      "Loading images for  data-224/train_resize_20/HTC-1-M7\n",
      "Loading images for  data-224/train_resize_20/LG-Nexus-5x\n",
      "Loading images for  data-224/train_resize_20/Motorola-Droid-Maxx\n",
      "Loading images for  data-224/train_resize_20/Motorola-Nexus-6\n",
      "Loading images for  data-224/train_resize_20/Motorola-X\n",
      "Loading images for  data-224/train_resize_20/Samsung-Galaxy-Note3\n",
      "Loading images for  data-224/train_resize_20/Samsung-Galaxy-S4\n",
      "Loading images for  data-224/train_resize_20/Sony-NEX-7\n",
      "Loading images for  data-224/train_resize_20/iPhone-4s\n",
      "Loading images for  data-224/train_resize_20/iPhone-6\n",
      "Loading images for  data-224/train_compressed_90/HTC-1-M7\n",
      "Loading images for  data-224/train_compressed_90/LG-Nexus-5x\n",
      "Loading images for  data-224/train_compressed_90/Motorola-Droid-Maxx\n",
      "Loading images for  data-224/train_compressed_90/Motorola-Nexus-6\n",
      "Loading images for  data-224/train_compressed_90/Motorola-X\n",
      "Loading images for  data-224/train_compressed_90/Samsung-Galaxy-Note3\n",
      "Loading images for  data-224/train_compressed_90/Samsung-Galaxy-S4\n",
      "Loading images for  data-224/train_compressed_90/Sony-NEX-7\n",
      "Loading images for  data-224/train_compressed_90/iPhone-4s\n",
      "Loading images for  data-224/train_compressed_90/iPhone-6\n",
      "Loading images for  data-224/train_resize_15/HTC-1-M7\n",
      "Loading images for  data-224/train_resize_15/LG-Nexus-5x\n",
      "Loading images for  data-224/train_resize_15/Motorola-Droid-Maxx\n",
      "Loading images for  data-224/train_resize_15/Motorola-Nexus-6\n",
      "Loading images for  data-224/train_resize_15/Motorola-X\n",
      "Loading images for  data-224/train_resize_15/Samsung-Galaxy-Note3\n",
      "Loading images for  data-224/train_resize_15/Samsung-Galaxy-S4\n",
      "Loading images for  data-224/train_resize_15/Sony-NEX-7\n",
      "Loading images for  data-224/train_resize_15/iPhone-4s\n",
      "Loading images for  data-224/train_resize_15/iPhone-6\n"
     ]
    }
   ],
   "source": [
    "images, labels = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Good ol' tandem shuffle. \"\"\"\n",
    "def shuffle(a, b, c=None):\n",
    "    assert len(a) == len(b)\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "    if c is not None:\n",
    "        assert len(c) == len(b)\n",
    "        np.random.set_state(state)\n",
    "        np.random.shuffle(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot (using tf for fun):\n",
    "one_hot_graph = tf.Graph()\n",
    "one_hot_session = tf.Session(graph=one_hot_graph)\n",
    "with one_hot_graph.as_default():\n",
    "    label_input = tf.placeholder(tf.int32)\n",
    "    one_hot = tf.one_hot(label_input, 10)\n",
    "label_one_hot = np.asarray(one_hot.eval(feed_dict={label_input: labels}, session=one_hot_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19800, 224, 224, 3) (19800, 10)\n",
      "(4950, 224, 224, 3) (4950, 10)\n"
     ]
    }
   ],
   "source": [
    "validation_ratio = int(len(images) * 0.8)\n",
    "shuffle(images, label_one_hot)  # Shuffle before dividing into training/validation sets\n",
    "\n",
    "train_data, train_labels = images[:validation_ratio], label_one_hot[:validation_ratio]\n",
    "valid_data, valid_labels = images[validation_ratio:], label_one_hot[validation_ratio:]\n",
    "training_samples = len(train_data)\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(valid_data.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Some convenience functions for creating layers\n",
    "def std(inputs):\n",
    "    return 1 / math.sqrt(inputs)\n",
    "def conv2d(x, w, strides=[1,1,1,1], padding='SAME'):\n",
    "    return tf.nn.conv2d(x, w, strides=strides, padding=padding)\n",
    "\n",
    "def max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME'):\n",
    "    return tf.nn.max_pool(x, ksize=ksize, strides=strides, padding=padding)\n",
    "####################################\n",
    "# calculating validation accuracy \n",
    "def total_correct(correct_prediction, valid_data, batch_size):    \n",
    "    correct_prediction.eval(feed_dict={X: batch[0], y_: batch[1], keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "shape_in = train_data.shape[1:]\n",
    "print(shape_in)\n",
    "conv1_filter_shape = [5, 5, 3]\n",
    "conv2_filter_shape = [5, 5]\n",
    "\n",
    "# Training Config\n",
    "batches = 1000\n",
    "batch_size = 100\n",
    "\n",
    "# Logging\n",
    "show_train_accuracy = 25\n",
    "show_test_accuracy = 50\n",
    "\n",
    "# Network Config\n",
    "conv1_features = 32\n",
    "conv2_features = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Network\n",
    "This is basically the same network I used when exploring expression recognition and speech commands. Training for a couple thousand batches will get it to around 98%/65% training/validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 112, 112, 32]\n",
      "step 0, training accuracy 0.1\n",
      "validation accuracy 0.07\n",
      "step 25, training accuracy 0.12\n",
      "step 50, training accuracy 0.06\n",
      "validation accuracy 0.15\n",
      "step 75, training accuracy 0.13\n",
      "step 100, training accuracy 0.15\n",
      "validation accuracy 0.16\n",
      "step 125, training accuracy 0.11\n",
      "step 150, training accuracy 0.12\n",
      "validation accuracy 0.1\n",
      "step 175, training accuracy 0.09\n",
      "Shuffling training data\n",
      "step 200, training accuracy 0.15\n",
      "validation accuracy 0.15\n",
      "step 225, training accuracy 0.14\n",
      "step 250, training accuracy 0.12\n",
      "validation accuracy 0.17\n",
      "step 275, training accuracy 0.22\n",
      "step 300, training accuracy 0.27\n",
      "validation accuracy 0.26\n",
      "step 325, training accuracy 0.13\n",
      "step 350, training accuracy 0.2\n",
      "validation accuracy 0.23\n",
      "step 375, training accuracy 0.23\n",
      "Shuffling training data\n",
      "step 400, training accuracy 0.24\n",
      "validation accuracy 0.29\n",
      "step 425, training accuracy 0.31\n",
      "step 450, training accuracy 0.36\n",
      "validation accuracy 0.36\n",
      "step 475, training accuracy 0.22\n",
      "step 500, training accuracy 0.49\n",
      "validation accuracy 0.35\n",
      "step 525, training accuracy 0.39\n",
      "step 550, training accuracy 0.34\n",
      "validation accuracy 0.38\n",
      "step 575, training accuracy 0.47\n",
      "Shuffling training data\n",
      "step 600, training accuracy 0.47\n",
      "validation accuracy 0.45\n",
      "step 625, training accuracy 0.36\n",
      "step 650, training accuracy 0.55\n",
      "validation accuracy 0.44\n",
      "step 675, training accuracy 0.48\n",
      "step 700, training accuracy 0.52\n",
      "validation accuracy 0.43\n",
      "step 725, training accuracy 0.41\n",
      "step 750, training accuracy 0.54\n",
      "validation accuracy 0.45\n",
      "step 775, training accuracy 0.6\n",
      "Shuffling training data\n",
      "step 800, training accuracy 0.58\n",
      "validation accuracy 0.46\n",
      "step 825, training accuracy 0.6\n",
      "step 850, training accuracy 0.6\n",
      "validation accuracy 0.47\n",
      "step 875, training accuracy 0.56\n",
      "step 900, training accuracy 0.68\n",
      "validation accuracy 0.5\n",
      "step 925, training accuracy 0.67\n",
      "step 950, training accuracy 0.59\n",
      "validation accuracy 0.45\n",
      "step 975, training accuracy 0.67\n",
      "Shuffling training data\n",
      "validation accuracy 0.48\n",
      "Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the network\n",
    "network_graph = tf.Graph();\n",
    "network_session = tf.Session(graph=network_graph)\n",
    "with network_graph.as_default():\n",
    "    \n",
    "    # Input images and labels\n",
    "    X = tf.placeholder(tf.float32, [None, shape_in[0], shape_in[1], shape_in[2]])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    # Conv Layer 1\n",
    "    w_conv1 = tf.Variable(tf.truncated_normal([conv1_filter_shape[0], conv1_filter_shape[1], conv1_filter_shape[2], conv1_features], stddev=std(shape_in[0] * shape_in[1] * shape_in[2])))\n",
    "    b_conv1 = tf.Variable(tf.constant(0.1, shape=[conv1_features]))\n",
    "    h_conv1 = tf.nn.relu(conv2d(tf.reshape(X, [-1, shape_in[0], shape_in[1], shape_in[2]]), w_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool(h_conv1)\n",
    "    # Conv Layer 2\n",
    "    w_conv2 = tf.Variable(tf.truncated_normal([conv2_filter_shape[0], conv2_filter_shape[1], conv1_features, conv2_features], stddev=std(conv2_filter_shape[0] * conv2_filter_shape[1] * conv2_features)))\n",
    "    b_conv2 = tf.Variable(tf.constant(0.1, shape=[conv2_features]))\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "\n",
    "    h_conv2_shape = h_conv2.get_shape().as_list()\n",
    "    hc2shape_product = h_conv2_shape[1] * h_conv2_shape[2] *h_conv2_shape[3]\n",
    "    h_conv2_flat = tf.reshape(h_conv2, [-1, hc2shape_product])\n",
    "    print(h_conv2_shape)\n",
    "    \n",
    "    # FC Layer 1\n",
    "    w_fc1 = tf.Variable(tf.truncated_normal([hc2shape_product, 128], stddev=std(hc2shape_product)))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv2_flat, w_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    # FC Layer 2\n",
    "    w_fc2 = tf.Variable(tf.truncated_normal([128, 128], stddev=std(128*128)))\n",
    "    b_fc2 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "    # FC Layer 3 (Output Layer)\n",
    "    w_fc3 = tf.Variable(tf.truncated_normal([128, 10], stddev=std(128*10)))\n",
    "    b_fc3 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "    y_conv = tf.matmul(h_fc2_drop, w_fc3) + b_fc3\n",
    "\n",
    "    softmax = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv)\n",
    "    cross_entropy = tf.reduce_mean(softmax)\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "# 2. Train the Network\n",
    "with network_session as sess:\n",
    "    if os.listdir('tf_checkpoints'):\n",
    "        print('Checkpoint found. Loading...')\n",
    "        saver.restore(sess, 'tf_checkpoints/model.ckpt')\n",
    "    else:    \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    index = 0\n",
    "    for i in range(batches):\n",
    "        \n",
    "        # Shuffle the data\n",
    "        if (index + 2 * batch_size) > training_samples :  # Just ignore the partial batch, whateverrrr\n",
    "            print('Shuffling training data')\n",
    "            shuffle(train_data, train_labels)\n",
    "            index = 0\n",
    "        else:\n",
    "            index += batch_size\n",
    "        batch = (train_data[index:index+batch_size], train_labels[index:index+batch_size])\n",
    "        train_step.run(feed_dict={X: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "        # 2.A Test the Network\n",
    "        if i % show_train_accuracy == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "              X: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        if i % show_test_accuracy == 0:\n",
    "            print('validation accuracy %g' % accuracy.eval(feed_dict={\n",
    "                  X: valid_data[:batch_size], y_: valid_labels[:batch_size], keep_prob: 1.0}))\n",
    "            \n",
    "    # Show Testing and Validation Accuracy at the end\n",
    "    # 2.B Validate the Network\n",
    "    print('validation accuracy %g' % accuracy.eval(feed_dict={\n",
    "        X: valid_data[:batch_size], y_: valid_labels[:batch_size], keep_prob: 1.0}))\n",
    "    \n",
    "    validation_result = correct_prediction.eval(feed_dict={X: valid_data[:batch_size], y_: valid_labels[:batch_size], keep_prob: 1.0})\n",
    "    validation_output = y_conv.eval(feed_dict={X: valid_data[:batch_size], y_: valid_labels[:batch_size], keep_prob: 1.0})\n",
    "    print('Saving checkpoint')\n",
    "    saver.save(sess, 'tf_checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['std', 'shuffle', 'load']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Deeper\n",
    "Next up, let's move on and make the network not only deeper, but residual as well.\n",
    "\n",
    "Let's make some mods!\n",
    "\n",
    "## Helper Function\n",
    "First off, let's make a convenient way of generating multiple convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, filter_shape, out_features):\n",
    "    input_shape = input_tensor.get_shape().as_list()\n",
    "    w_conv = tf.Variable(tf.truncated_normal([filter_shape[0], filter_shape[1], input_shape[-1], out_features], stddev=std(filter_shape[0] * filter_shape[1] * out_features)))\n",
    "    b_conv = tf.Variable(tf.constant(0.1, shape=[out_features]))\n",
    "    h_conv = tf.nn.relu(conv2d(input_tensor, w_conv) + b_conv)\n",
    "    return h_conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "shape_in = train_data.shape[1:]\n",
    "print(shape_in)\n",
    "conv1_filter_shape = [7, 7]\n",
    "conv2_filter_shape = [3, 3]\n",
    "conv3_filter_shape = [3, 3]\n",
    "conv4_filter_shape = [3, 3]\n",
    "conv5_filter_shape = [3, 3]\n",
    "# Training Config\n",
    "batches = 100\n",
    "batch_size = 50\n",
    "\n",
    "# Logging\n",
    "show_train_accuracy = 25\n",
    "show_test_accuracy = 50\n",
    "\n",
    "# Network Config\n",
    "conv1_features = 64\n",
    "conv2_features = 64\n",
    "conv3_features = 128\n",
    "conv4_features = 256\n",
    "conv5_features = 512\n",
    "conv2_layers = 6  # Conv1 is always 1 layer in our case\n",
    "conv3_layers = 8\n",
    "conv4_layers = 12\n",
    "conv5_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(tensor, data, labels, batch_size=100):\n",
    "    results = []\n",
    "    total = 0.0\n",
    "    for start_index in range(0, batch_size, batch_size): # len(data), batch_size):  # just doing samples of 100 because it takes forever\n",
    "        end_index = min(start_index + 100, len(data))\n",
    "        result = tensor.eval(feed_dict={\n",
    "                  X: data[start_index:end_index], y_: labels[start_index:end_index], keep_prob: 1.0})\n",
    "        # scale if smaller than batch size\n",
    "        results += [result * (end_index - start_index) / batch_size]\n",
    "    return np.average(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not Residual Yet\n",
    "Here's a basic, deep network without the residual part of a ResNet. It...struggles to get anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool Shape:  [None, 112, 112, 64]\n",
      "Final pool shape:  [None, 14, 14, 512]\n",
      "Checkpoint found. Loading...\n",
      "INFO:tensorflow:Restoring parameters from tf_checkpoints/model.ckpt\n",
      "step 0, training accuracy 0.06\n",
      "validation accuracy 0.14\n",
      "step 25, training accuracy 0.16\n",
      "step 50, training accuracy 0.08\n",
      "validation accuracy 0.12\n",
      "step 75, training accuracy 0.14\n",
      "Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Define the network\n",
    "network_graph = tf.Graph();\n",
    "network_session = tf.Session(graph=network_graph)\n",
    "with network_graph.as_default():\n",
    "    \n",
    "    # Input images and labels\n",
    "    X = tf.placeholder(tf.float32, [None, shape_in[0], shape_in[1], shape_in[2]])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    # Conv Layer 1\n",
    "    h_conv1 = conv_block(tf.reshape(X, [-1, shape_in[0], shape_in[1], shape_in[2]]), conv1_filter_shape, conv1_features)\n",
    "    h_pool1 = max_pool(h_conv1)\n",
    "    # Conv Layer 2\n",
    "    print('Pool Shape: ', h_pool1.get_shape().as_list())\n",
    "    h_conv2 = h_pool1  # A little hacky, but necessary to start the next block\n",
    "    for i in range(conv2_layers):        \n",
    "        h_conv2 = conv_block(h_conv2, conv2_filter_shape, conv2_features)\n",
    "    h_pool2 = max_pool(h_conv2)\n",
    "    # Conv Layer 3\n",
    "    h_conv3 = h_pool2  # Again, a little hacky\n",
    "    for i in range(conv3_layers):\n",
    "        h_conv3 = conv_block(h_conv3, conv3_filter_shape, conv3_features)\n",
    "    h_pool3 = max_pool(h_conv3)\n",
    "    # Conv Layer 4\n",
    "    h_conv4 = h_pool3  # Again, a little hacky\n",
    "    for i in range(conv4_layers):\n",
    "        h_conv4 = conv_block(h_conv4, conv4_filter_shape, conv4_features)\n",
    "    h_pool4 = max_pool(h_conv4)\n",
    "    h_conv5 = h_pool4  # Again, a little hacky\n",
    "    for i in range(conv5_layers):\n",
    "        h_conv5 = conv_block(h_conv5, conv5_filter_shape, conv5_features)\n",
    "    \n",
    "    h_conv5_shape = h_conv5.get_shape().as_list()\n",
    "    hc5shape_product = h_conv5_shape[1] * h_conv5_shape[2] * h_conv5_shape[3]\n",
    "    h_conv5_flat = tf.reshape(h_conv5, [-1, hc5shape_product])\n",
    "    print('Final pool shape: ', h_conv5_shape)\n",
    "    \n",
    "    # FC Layer 1\n",
    "    #w_fc1 = tf.Variable(tf.truncated_normal([hp2shape_product, 1024], stddev=std(hp2shape_product)))\n",
    "    w_fc1 = tf.Variable(tf.truncated_normal([hc5shape_product, 256], stddev=std(hc5shape_product)))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[256]))\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, w_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    # FC Layer 2\n",
    "    w_fc2 = tf.Variable(tf.truncated_normal([256, 256], stddev=std(256*256)))\n",
    "    b_fc2 = tf.Variable(tf.constant(0.1, shape=[256]))\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "    h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "    # FC Layer 3 (Output Layer)\n",
    "    w_fc3 = tf.Variable(tf.truncated_normal([256, 10], stddev=std(256*10)))\n",
    "    b_fc3 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "    y_conv = tf.matmul(h_fc2_drop, w_fc3) + b_fc3\n",
    "\n",
    "    softmax = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv)\n",
    "    cross_entropy = tf.reduce_mean(softmax)\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "# 2. Train the Network\n",
    "with network_session as sess:\n",
    "    if os.listdir('tf_checkpoints'):\n",
    "        print('Checkpoint found. Loading...')\n",
    "        saver.restore(sess, 'tf_checkpoints/model.ckpt')\n",
    "    else:    \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    index = 0\n",
    "    for i in range(batches):\n",
    "        \n",
    "        # Shuffle the data\n",
    "        if (index + 2 * batch_size) > training_samples :  # Just ignore the partial batch, whateverrrr\n",
    "            print('Shuffling training data')\n",
    "            shuffle(train_data, train_labels)\n",
    "            index = 0\n",
    "        else:\n",
    "            index += batch_size\n",
    "        batch = (train_data[index:index+batch_size], train_labels[index:index+batch_size])\n",
    "        \n",
    "        train_step.run(feed_dict={X: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "        # 2.A Test the Network\n",
    "        if i % show_train_accuracy == 0:\n",
    "            #train_accuracy = calculate_accuracy(accuracy, train_data, train_labels, batch_size)\n",
    "            \n",
    "            print('step %d, training accuracy %g' % (i, accuracy.eval(feed_dict={X: batch[0], y_: batch[1], keep_prob: 1.0})))\n",
    "        if i % show_test_accuracy == 0:\n",
    "            #print('validation accuracy %g' % calculate_accuracy(accuracy, valid_data, valid_labels, batch_size))\n",
    "            print('validation accuracy %g' % accuracy.eval(feed_dict={X: valid_data[:batch_size], y_: valid_labels[:batch_size], keep_prob: 1.0}))\n",
    "            \n",
    "    # Show Testing and Validation Accuracy at the end\n",
    "    # 2.B Validate the Network\n",
    "    #print('validation accuracy %g' % calculate_accuracy(accuracy, valid_data, valid_labels, batch_size))\n",
    "    accuracy.eval(feed_dict={X: valid_data[:batch_size], y_: valid_labels[:batch_size], keep_prob: 1.0})\n",
    "    \n",
    "    validation_result = correct_prediction.eval(feed_dict={X: valid_data[:batch_size], y_: valid_labels[:batch_size], keep_prob: 1.0})\n",
    "    validation_output = y_conv.eval(feed_dict={X: valid_data[:batch_size], y_: valid_labels[:batch_size], keep_prob: 1.0})\n",
    "    print('Saving checkpoint')\n",
    "    saver.save(sess, 'tf_checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Residual\n",
    "To build the ResNet graph, we need to divide it up into chunks (in a more appropriate way than above). First, let's break down each part of the ResNet model (to match up with Table 1 of the [original paper](https://arxiv.org/pdf/1512.03385.pdf):  \n",
    "Input:  \n",
    "* 224x224x3  \n",
    "\n",
    "### First Conv Layer\n",
    "This one is a bit of an exception compared to the conv layers following it.  \n",
    "* Conv - 7x7, stride 2, 64 feature output  \n",
    "* BatchNorm & Scale - Need to set weight decay for BatchNorm, then scale (normalize) back up to 255\n",
    "* ReLU activation  \n",
    "Pool:  \n",
    "* Max Pool - Use a 3x3 kernel with a stride of 2  \n",
    "Output:  \n",
    "* 56x56x64 (note the stride=2 for the convolution)  \n",
    "\n",
    "### Bottleneck Blocks  \n",
    "Each block of the ResNet structure consists of a convolution, batch normalization, and scaling, which is repeated 3, 4, or 6 times. Additionally, the first part of each block includes a projection of the output from the previous layer (every other part of the block will simply take the output of the previous part of the block). So, each block will look like this:\n",
    "\n",
    "#### Projection\n",
    "This is the easy part. For the first activation of a block, we'll need to project the previous block (or pool) to have enough features. Simply perform a 1x1 convolution with 4 times the number of input features followed by the typical batch normalization and scaling.\n",
    "\n",
    "#### Bottleneck Operation\n",
    "Perform a set of 3 convolutions (each followed by batch norm and scaling and ReLU, except for the final one). The convolution sizes should be:  \n",
    "* 1x1xinput_features  \n",
    "* 3x3xinput_features  \n",
    "* 1x1x4(input_features)  \n",
    "\n",
    "Once the output of the final convolution (and BN & scaling) is calculated, perform an **elementwise addition** between this output and the one from the projection. Then, **run the result through the ReLU activation**.  \n",
    "\n",
    "**For every bottleneck operation beyond the first** (that is, before we have another convolution with a stride of 2 and multiply the features by 4 again), perform another bottleneck operation and simply (elementwise-)add the output of the previous bottleneck operation to it before performing the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Some convenience functions for creating layers\n",
    "\n",
    "def conv2d(x, w, stride=1, padding='SAME'):\n",
    "    return tf.nn.conv2d(x, w, strides=[1, stride, stride, 1], padding=padding)\n",
    "\n",
    "def max_pool(x, ksize=2, stride=2, padding='SAME'):\n",
    "    return tf.nn.max_pool(x, ksize=[1, ksize, ksize, 1], strides=[1, stride, stride, 1], padding=padding)\n",
    "\n",
    "# ResNet functions\n",
    "def conv_batch_scale():\n",
    "    conv2d\n",
    "    tf.contrib.layers.batch_norm()\n",
    "def bottleneck_block(tensor_in):\n",
    "    \n",
    "####################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
