{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building ResNet50 From Scratch (sort-of)\n",
    "I wanted to become more familiar with the structure of a Deep Residual Network and so decided to define my own using Keras. I am aware that Keras [already has a ResNet50 model](https://keras.io/applications/#resnet50), but there are a few reasons to build my own:  \n",
    "- I want to replace the Fully-Connected layer with my own 10-node layer (instead of appending it)\n",
    "- I've never used a functional model in Keras, so it's good practice\n",
    "- I'll have a better appreciation for developing a deep network  \n",
    "\n",
    "Of course, I'll need something to train it on and for that, I'll be using it for my entries to a Kaggle competition for [identifying camera models from pictures](https://www.kaggle.com/c/sp-society-camera-model-identification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "First, some standard library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "For the competition, we have 10 camera models and a directory structure like this:\n",
    "- data\n",
    "- - HTC-1-M7\n",
    "- - LG-Nexus-5x\n",
    "- - ...\n",
    "\n",
    "The images could also be manipulated in a number of ways, so from this, I have generated a new dataset as follows:\n",
    "- data-512\n",
    "- - train_cropped\n",
    "- - - HTC-1-M7\n",
    "- - - ...\n",
    "- - train_compressed_70\n",
    "- - - HTC-1-M7\n",
    "- - - ...\n",
    "- - train_resize_5\n",
    "- - ...  \n",
    "\n",
    "Basically, I just have a dedicated folder for each type of photo manipulation (also, data-512 indicates that all of the images I'm using for training have been center-cropped to be 512x512, this may change at some point). Below, we'll define the labels to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CameraLabel(Enum):\n",
    "    HTC_1_M7 = 0\n",
    "    LG_Nexus_5x = 1\n",
    "    Motorola_Droid_Maxx = 2\n",
    "    Motorola_Nexus_6 = 3\n",
    "    Motorola_X = 4\n",
    "    Samsung_Galaxy_Note3 = 5\n",
    "    Samsung_Galaxy_S4 = 6\n",
    "    Sony_NEX_7 = 7\n",
    "    iPhone_4s = 8\n",
    "    iPhone_6 = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the Data\n",
    "While the images are 512x512, the network will accept a 224x224 image. To that end, I've created two basic functions for retrieving crops of the training data; *center_crop* does as you'd expect and *crop_224* will retrieve *crops_per_image* for each image passed in. Each of the crops in *crop_224* will be from a random position within the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Crops a randomly-selected 224x224x3 image from a larger image\n",
    "'''\n",
    "def crop_224(images, crops_per_image=1):\n",
    "    out_images = []\n",
    "    for image in images:\n",
    "        row_index = np.random.randint(0, image.shape[0] - 224)\n",
    "        col_index = np.random.randint(0, image.shape[1] - 224)\n",
    "        out_images += [image[row_index:row_index + 224, col_index:col_index + 224, :]]\n",
    "    return np.asarray(out_images)\n",
    "\n",
    "def center_crop(img, crop_dim=224):\n",
    "    edge = crop_dim // 2\n",
    "    height, width, _ = img.shape\n",
    "    center_height = height // 2\n",
    "    center_width = width // 2\n",
    "    top = center_height - edge\n",
    "    bottom = center_height + edge\n",
    "    left = center_width - edge\n",
    "    right = center_width + edge\n",
    "    return img[top:bottom,left:right]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "Of course, we need to load the data to train our network. Now, I can't load all of the data at once, because even at 512x512, that takes at least 13GB of memory, so I'll simply introduce a *load_ratio* parameter so I can load a subset of the data for each epoch. I'm also being lazy with my development and just use a separate function for loading the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path='data-224', load_ratio=1.0, image_dim=224, verbose=False):\n",
    "    print('Loading images')\n",
    "    data = []\n",
    "    labels = []\n",
    "    # Read data from every type of generated data\n",
    "    for data_path in os.listdir(path):\n",
    "        full_data_path = os.path.join(path, data_path)\n",
    "        # Read data from every camera type\n",
    "        for label in CameraLabel:\n",
    "            camera_path = os.path.join(full_data_path, label.name.replace('_', '-'))\n",
    "            if verbose:\n",
    "                print(\"Loading images for \", camera_path)\n",
    "            filenames = os.listdir(camera_path)\n",
    "            last_index = int(load_ratio * len(filenames))\n",
    "            for filename in filenames[:last_index]:\n",
    "                img = cv2.imread(os.path.join(camera_path, filename))\n",
    "                if img.shape[0] != image_dim or img.shape[1] != image_dim or img.shape[2] != 3:\n",
    "                    print('Image with invalid shape! Shape: ', img.shape, 'Name: ', os.path.join(camera_path, filename))\n",
    "                    continue\n",
    "                data += [img]\n",
    "                labels += [CameraLabel[label.name].value]\n",
    "    return np.asarray(data), np.expand_dims(np.asarray(labels), axis=1)\n",
    "\n",
    "def load_validation(path='data/validation', needs_crop=False, verbose=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "    # Read data from every camera type\n",
    "    for label in CameraLabel:\n",
    "        camera_path = os.path.join(path, label.name.replace('_', '-'))\n",
    "        if verbose:\n",
    "            print(\"Loading images for \", camera_path)\n",
    "        for filename in os.listdir(camera_path):\n",
    "            img = cv2.imread(os.path.join(camera_path, filename))\n",
    "            if needs_crop:\n",
    "                img = center_crop(img)\n",
    "            if img.shape[0] != 224 or img.shape[1] != 224 or img.shape[2] != 3:\n",
    "                print('Loaded an image with improper dimensions. Discarding')\n",
    "            else:\n",
    "                data += [img]\n",
    "                labels += [CameraLabel[label.name].value]\n",
    "            del img\n",
    "    return np.asarray(data), np.expand_dims(np.asarray(labels), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and load the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images for  data/flickr-validation/HTC-1-M7\n",
      "Loading images for  data/flickr-validation/LG-Nexus-5x\n",
      "Loading images for  data/flickr-validation/Motorola-Droid-Maxx\n",
      "Loading images for  data/flickr-validation/Motorola-Nexus-6\n",
      "Loaded an image with improper dimensions. Discarding\n",
      "Loading images for  data/flickr-validation/Motorola-X\n",
      "Loading images for  data/flickr-validation/Samsung-Galaxy-Note3\n",
      "Loading images for  data/flickr-validation/Samsung-Galaxy-S4\n",
      "Loading images for  data/flickr-validation/Sony-NEX-7\n",
      "Loading images for  data/flickr-validation/iPhone-4s\n",
      "Loading images for  data/flickr-validation/iPhone-6\n",
      "Loaded  14583 images\n"
     ]
    }
   ],
   "source": [
    "valid_images, valid_labels = load_validation('data/flickr-validation', needs_crop=True, verbose=True)\n",
    "print('Loaded ', valid_images.shape[0], 'images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network\n",
    "Ok! Time to build this network. I have built it based on this [netscope graph](https://dgschwend.github.io/netscope/#/preset/resnet-50) with the main exception being the final fully-connected layer. Also, of course, I've referenced the now-famous paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385).  \n",
    "\n",
    "### A Note About Weights\n",
    "I haven't spent much time researching weight initialization for the network outside of reading [some notes](http://cs231n.github.io/neural-networks-2/#init), but I'm following TensorFlow's practice of using *truncated normals* to initialize the weights of my network. From my experience, it works well for ReLU-based networks both with and without batch normalization, so I'll keep that in here for now. We'll need a simple standard deviation function to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(inputs):\n",
    "    return 1 / math.sqrt(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Network\n",
    "In case there's an already-existing network I'd like to work with, I'll go ahead and load it here. Otherwise, we'll use the network definition after this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model to work with\n",
    "model_filename = 'keras_models/second_resnet.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously-created model, keras_models/second_resnet.h5\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(model_filename):\n",
    "    print('Loading previously-created model,', model_filename)\n",
    "    model = keras.models.load_model(model_filename)\n",
    "else:\n",
    "    print('No model found for path: ', model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Network\n",
    "If you've loaded a network in the previous section, then skip to the section on Training.  \n",
    "\n",
    "So, I need a fresh network sometimes and here's where I'll define it. Looking over the netscope graph and the paper, the key building *block* to building a ResNet model is what's called the *bottleneck*. It is a series of 1x1, 3x3, and 1x1 convolutions with batch normalization and ReLU activations between them (**not at the end of it, however**) with various parameters to maintain shape. To make my life easier, I'm going to define the *bottleneck_block* function to build it for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The bottleneck block of a resnet has the following structure:\n",
    "    1x1 conv on the input features (no bias, no pad, 1 stride), batch norm w/ scaling, and followed by ReLU activation\n",
    "    3x3 conv w/ no bias, 1 pad, 1 stride, batch norm w/ scaling, ReLU follows\n",
    "    1x1 conv w/ 4x input features (no bias, no pad, 1 stride), batch norm w/ scaling, NO ACTIVATION\n",
    "'''\n",
    "def bottlenet_block(input_net, input_filters=64, down_sampling=False, feature_size=56, middle_filters_divisor=2, output_filters_scale=1):\n",
    "    \n",
    "    first_stride = 2 if down_sampling else 1\n",
    "    # if I want to initialize weights, I need to know incoming size, figure that out\n",
    "    \n",
    "    # First Convolution\n",
    "    block = keras.layers.Conv2D(filters=input_filters, kernel_size=1, strides=first_stride, use_bias=False, kernel_initializer=keras.initializers.TruncatedNormal(stddev=std(input_filters * feature_size ** 2)))(input_net)\n",
    "    block = keras.layers.BatchNormalization()(block)\n",
    "    block = keras.layers.Activation('relu')(block)\n",
    "    # Second Convolution\n",
    "    block = keras.layers.Conv2D(filters=input_filters // middle_filters_divisor, kernel_size=3, padding='same', strides=1, use_bias=False, kernel_initializer=keras.initializers.TruncatedNormal(stddev=std(input_filters // middle_filters_divisor * (feature_size / first_stride) ** 2)))(block)\n",
    "    block = keras.layers.BatchNormalization()(block)\n",
    "    block = keras.layers.Activation('relu')(block)\n",
    "    # Final Convolution (no activation)\n",
    "    block = keras.layers.Conv2D(filters=input_filters * output_filters_scale, kernel_size=1, strides=1, use_bias=False, kernel_initializer=keras.initializers.TruncatedNormal(stddev=std(input_filters * output_filters_scale * (feature_size / first_stride) ** 2)))(block)\n",
    "    block = keras.layers.BatchNormalization()(block)\n",
    "    \n",
    "    return block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters above are a little confusing, so I'll detail them here.\n",
    "- input_net - The network we're attaching this bottleneck block to\n",
    "- input_filters - The number of filters/features coming into this block\n",
    "- down_sampling - Instead of pooling, ResNets just bump the convolution stride when the features are about to double\n",
    "- feature_size - Used for calculating weights during initialization. Far as I can tell, there's not an easy way to get the incoming filter/feature shape for a functional model with Keras until the model is actually instantiated. Kinda hacky\n",
    "- middle_filters_divisor - Used in tandem with output_filters_scale to determine the number of features in the middle convolution. Typically 2 or 4\n",
    "- output_filters_scale - Multiplies the incoming number of filters for the next layer. Typically is 1 or 2\n",
    "\n",
    "The last two parameters are the weird ones. If you look closely at the [netscope graph], you'll notice each *block* typically will reduce the number of filters by a factor of 4 and then restore it to its original size at the end, which would give us  `middle_filters_divisor=4` and `output_filters_scale=1`. However, at points in the network where the features will double, the middle layers of the block will only have the filters cut in *half* and then the output layer will *double* the number of features, so you'll see `middle_filters_divisor=2` and `output_filters_scale=2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Build It Already\n",
    "Ok, so the blocks defined above will be repeated *16 times* (3x16 gives me 48 layers, plus an initial conv-layer and output layer, hence ResNet**50**!). Yes, the definition that follows is long and could be rolled up into a loop, but I wanted to show exactly what the structure looks like; plus, I don't have to define any extra function parameters :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(224, 224, 3))\n",
    "\n",
    "# An initial convolutional layer with strides=2 and pooling to reduce the image size\n",
    "net = keras.layers.Conv2D(filters=64, kernel_size=7, strides=2, padding='same', input_shape=(224, 224, 3), kernel_initializer=keras.initializers.TruncatedNormal(stddev=std(224*224*3)))(inputs)\n",
    "net = keras.layers.BatchNormalization()(net)\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "net = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(net)\n",
    "\n",
    "##########################################\n",
    "# First Set of Bottlenecks\n",
    "bottle_1 = bottlenet_block(net, input_filters=64, feature_size=56, middle_filters_divisor=1, output_filters_scale=4)\n",
    "# Projection Function\n",
    "net = keras.layers.Conv2D(filters=256, kernel_size=1, use_bias=False, strides=1, kernel_initializer=keras.initializers.TruncatedNormal(stddev=std(56*56*256)))(net)\n",
    "net = keras.layers.BatchNormalization()(net)\n",
    "# Merge Bottleneck and Projection\n",
    "net = keras.layers.Add()([bottle_1, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "bottle_2 = bottlenet_block(net, input_filters=256, feature_size=56, middle_filters_divisor=4, output_filters_scale=1)\n",
    "net = keras.layers.Add()([bottle_2, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "bottle_3 = bottlenet_block(net, input_filters=256, feature_size=56, middle_filters_divisor=4, output_filters_scale=1)\n",
    "net = keras.layers.Add()([bottle_3, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "##########################################\n",
    "# Second Set of Bottlenecks\n",
    "bottle_4 = bottlenet_block(net, input_filters=256, down_sampling=True, feature_size=56, middle_filters_divisor=2, output_filters_scale=2)\n",
    "# Projection Function\n",
    "net = keras.layers.Conv2D(filters=512, kernel_size=1, use_bias=False, strides=2, kernel_initializer=keras.initializers.TruncatedNormal(stddev=std(28*28*512)))(net)\n",
    "net = keras.layers.BatchNormalization()(net)\n",
    "# Merge Bottleneck and Projection\n",
    "net = keras.layers.Add()([bottle_4, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "bottle_5 = bottlenet_block(net, input_filters=512, feature_size=28, middle_filters_divisor=4, output_filters_scale=1)\n",
    "net = keras.layers.Add()([bottle_5, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "bottle_6 = bottlenet_block(net, input_filters=512, feature_size=28, middle_filters_divisor=4, output_filters_scale=1)\n",
    "net = keras.layers.Add()([bottle_6, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "bottle_7 = bottlenet_block(net, input_filters=512, feature_size=28, middle_filters_divisor=4, output_filters_scale=1)\n",
    "net = keras.layers.Add()([bottle_7, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "##########################################\n",
    "# Third Set of Bottlenecks\n",
    "bottle_8 = bottlenet_block(net, input_filters=512, down_sampling=True, feature_size=28, middle_filters_divisor=2, output_filters_scale=2)\n",
    "# Projection Function\n",
    "net = keras.layers.Conv2D(filters=1024, kernel_size=1, use_bias=False, strides=2, kernel_initializer=keras.initializers.TruncatedNormal(stddev=std(14*14*1024)))(net)\n",
    "net = keras.layers.BatchNormalization()(net)\n",
    "# Merge Bottleneck and Projection\n",
    "net = keras.layers.Add()([bottle_8, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "bottle_9 = bottlenet_block(net, input_filters=1024, feature_size=14, middle_filters_divisor=4, output_filters_scale=1)\n",
    "net = keras.layers.Add()([bottle_9, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "bottle_10 = bottlenet_block(net, input_filters=1024, feature_size=14, middle_filters_divisor=4, output_filters_scale=1)\n",
    "net = keras.layers.Add()([bottle_10, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "bottle_11 = bottlenet_block(net, input_filters=1024, feature_size=14, middle_filters_divisor=4, output_filters_scale=1)\n",
    "net = keras.layers.Add()([bottle_11, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "bottle_12 = bottlenet_block(net, input_filters=1024, feature_size=14, middle_filters_divisor=4, output_filters_scale=1)\n",
    "net = keras.layers.Add()([bottle_12, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "bottle_13 = bottlenet_block(net, input_filters=1024, feature_size=14, middle_filters_divisor=4, output_filters_scale=1)\n",
    "net = keras.layers.Add()([bottle_13, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "##########################################\n",
    "# Fourth Set of Bottlenecks\n",
    "bottle_14 = bottlenet_block(net, input_filters=1024, down_sampling=True, feature_size=14, middle_filters_divisor=2, output_filters_scale=2)\n",
    "# Projection Function\n",
    "net = keras.layers.Conv2D(filters=2048, kernel_size=1, use_bias=False, strides=2, kernel_initializer=keras.initializers.TruncatedNormal(stddev=std(7*7*2048)))(net)\n",
    "net = keras.layers.BatchNormalization()(net)\n",
    "# Merge Bottleneck and Projection\n",
    "net = keras.layers.Add()([bottle_14, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "bottle_15 = bottlenet_block(net, input_filters=2048, feature_size=7, middle_filters_divisor=4, output_filters_scale=1)\n",
    "net = keras.layers.Add()([bottle_15, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "bottle_16 = bottlenet_block(net, input_filters=2048, feature_size=7, middle_filters_divisor=4, output_filters_scale=1)\n",
    "net = keras.layers.Add()([bottle_16, net])\n",
    "net = keras.layers.Activation('relu')(net)\n",
    "\n",
    "# Final Pooling. Should have 2048 outputs\n",
    "net = keras.layers.AveragePooling2D(pool_size=(7,7))(net)\n",
    "\n",
    "# Final Fully-Connected Layer\n",
    "flatten_shape = 2048\n",
    "net = keras.layers.Flatten()(net)\n",
    "predictions = keras.layers.Dense(10, activation='softmax', kernel_initializer=keras.initializers.TruncatedNormal(stddev=std(10 * flatten_shape)))(net)\n",
    "\n",
    "model = keras.models.Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "Nothing extravagant going on here, just using the Adam optimizer and typical cross-entropy classification.  \n",
    "\n",
    "**Note: If loading a previously-trained model, this step is not necessary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Now, I have the model and it's time to train! Hold on to your butts, though; on a modest 4th gen i5 and a powerful GTX 1080, it will take about 10 minutes for an epoch to train, assuming ~25000 images in batches of 16.  \n",
    "\n",
    "First up, the training function. It's messy, I know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_and_metrics = model.evaluate(train_data, train_labels, batch_size=50)\n",
    "#model.fit(train_data, train_labels, epochs=5, batch_size=50)\n",
    "\n",
    "''' Crop & Train Idea:\n",
    "\n",
    "1. Load 25% of images, which will take about 8GB of memory \n",
    "2. Random crop 4 times for each image, giving 5 samples per image\n",
    "3. Train for an epoch\n",
    "4. Shuffle and repeat step 3 for an epoch if I feel like it (spend less time loading images. It's a bonus!)\n",
    "5. Repeat from beginning\n",
    "'''\n",
    "\n",
    "def train(model, data_path='data-512', load_ratio=0.25, validation_data=None, epochs=1, batch_size=16, crops_per_image=4, bonus_training=False):\n",
    "    \n",
    "    if validation_data is not None:\n",
    "        valid_images = validation_data[0]\n",
    "        valid_label_one_hot = tf.keras.utils.to_categorical(validation_data[1], num_classes=10)\n",
    "    else:\n",
    "        valid_images = None\n",
    "        valid_label_one_hot = None\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        images, labels = load(path=data_path, image_dim=512, load_ratio=load_ratio)\n",
    "        train_images = crop_224(images, crops_per_image=crops_per_image)\n",
    "        for j in range(1, crops_per_image):\n",
    "            train_images = np.concatenate((train_images, crop_224(images, crops_per_image=crops_per_image)))\n",
    "        \n",
    "        # Need to elementwise duplicate labels for crops_per_image > 1\n",
    "        labels = np.reshape(np.tile(labels[:,0], crops_per_image), (crops_per_image * len(labels), 1))\n",
    "        # One-hot-ify the labels\n",
    "        train_labels = tf.keras.utils.to_categorical(labels, num_classes=10)\n",
    "        \n",
    "        # using initial_epoch does not work for some reason, so all epochs will say 1/1 :(\n",
    "        if validation_data is not None:\n",
    "            model.fit(train_images, train_labels, epochs=1, batch_size=batch_size, validation_data=(valid_images, valid_label_one_hot), verbose=1)\n",
    "        else:\n",
    "            model.fit(train_images, train_labels, epochs=1, batch_size=batch_size, verbose=1)\n",
    "        # get some more mileage outta training before reloading\n",
    "        if bonus_training:\n",
    "            shuffle(train_images, train_labels)\n",
    "            model.fit(train_images, train_labels, epochs=1, batch_size=batch_size, validation_data=(valid_images, valid_label_one_hot), verbose=1)\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print('Saving model')\n",
    "            model.save(model_filename)\n",
    "        # I noticed some memory leakage, probably because of Jupyter, so I explicitly delete all of the generated data here\n",
    "        del images\n",
    "        del labels\n",
    "        del train_images\n",
    "        del train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get On With It and Train!\n",
    "Ok ok, time to finally train! Sit back and watch the numbers (slowly) go up :) Feel free to bump up the batch_size if the GPU has enough memory, it should speed things along a tiny bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n",
      "Epoch 1/1\n",
      "21816/21816 [==============================] - 386s 18ms/step - loss: 0.9381 - acc: 0.6762\n",
      "Loading images\n",
      "Epoch 1/1\n",
      "21816/21816 [==============================] - 382s 17ms/step - loss: 0.8987 - acc: 0.6892\n",
      "Loading images\n",
      "Epoch 1/1\n",
      "21816/21816 [==============================] - 383s 18ms/step - loss: 0.8741 - acc: 0.7040\n",
      "Loading images\n",
      "Epoch 1/1\n",
      "21816/21816 [==============================] - 383s 18ms/step - loss: 0.8526 - acc: 0.7070\n",
      "Loading images\n",
      "Epoch 1/1\n",
      "21816/21816 [==============================] - 383s 18ms/step - loss: 0.8056 - acc: 0.7238\n",
      "Saving model\n",
      "Loading images\n",
      "Epoch 1/1\n",
      "21816/21816 [==============================] - 384s 18ms/step - loss: 0.7967 - acc: 0.7265\n",
      "Loading images\n",
      "Epoch 1/1\n",
      "21816/21816 [==============================] - 384s 18ms/step - loss: 0.7630 - acc: 0.7403\n",
      "Loading images\n",
      "Epoch 1/1\n",
      "21816/21816 [==============================] - 384s 18ms/step - loss: 0.7184 - acc: 0.7542\n",
      "Loading images\n",
      "Epoch 1/1\n",
      "21816/21816 [==============================] - 385s 18ms/step - loss: 0.7025 - acc: 0.7585\n",
      "Loading images\n",
      "Epoch 1/1\n",
      "21816/21816 [==============================] - 384s 18ms/step - loss: 0.6650 - acc: 0.7687\n",
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "train(model, data_path='data-512-with-flickr', load_ratio=0.10, epochs=10, batch_size=32, crops_per_image=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "Keras makes this way too easy, it almost feels dirty. Change the filename if it suits you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "We have a trained model, now what do we do? Make a submission to Kaggle, of course! The code below reads each image in `data/test` one at a time and writes the prediction out to a submission file.  \n",
    "\n",
    "For added fun, I sample each test image 10 times (cropped at different spots) and just take the prediction with the highest confidence; I found this gives slightly better results. It's a naive approach and will certainly be improved later with proper ensembling, averaging, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Guide to making predictions:\n",
    "open file\n",
    "write header\n",
    "For each test image:\n",
    "    load the image\n",
    "    crop the image in multiple plocations\n",
    "    evaluate the image using the trained model and select label with highest confidence from all crops\n",
    "    write filename and category name to file\n",
    "save file\n",
    "close file\n",
    "submit file\n",
    "'''\n",
    "samples = 9\n",
    "test_dir = 'data/test'\n",
    "submission_filename = 'submissions/eighth_submission.csv'\n",
    "result = ['fname,camera']\n",
    "for filename in os.listdir(test_dir):\n",
    "    image = cv2.imread(os.path.join(test_dir, filename))\n",
    "    test_images = np.expand_dims(center_crop(image), axis=0)\n",
    "    for i in range(samples):\n",
    "        test_images = np.concatenate((test_images, crop_224([image])))\n",
    "    predictions = model.predict(test_images, batch_size=test_images.shape[0], verbose=0)\n",
    "    category = np.argmax(predictions) % 10\n",
    "    camera_name = CameraLabel(category).name.replace('_','-')\n",
    "    result += [filename + ',' + camera_name]\n",
    "    #print(result)\n",
    "    del image\n",
    "    del test_images\n",
    "result = np.asarray(result)\n",
    "\n",
    "np.savetxt(submission_filename, result, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
